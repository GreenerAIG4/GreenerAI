# GPT-3

Generative Pre-trained Transformer 3

## What is GPT-3
GPT-3 is the abbreviation of Generative Pre-Trained Transformer 3, it is created by OpenAI and it is the successor to GPT-2. Its beta version was released on June 11, 2020 and it was the largest language model ever(The previous largest is Turing NLG). It has 175 billion machine learning parameters gained through training on hundreds of billions of words or tokens(gained from Common Crawl, WebTex2, Books1 and Books2 and Wikipedia), while Turing NLG only has 17 billion parameters.

## What is GPT-3 used for?
Not It is used for various tasks, ranging from generating texts which even humans could not tell whether it is human written or computer generated to writing program code according to user input.

## GPT-3 Related Projects
##### GPT-3 creates a machine learning model - Matt Shumer
- Matt Schumer, CEO of OthersideAI, has used GPT-3 to generate code for a machine learning model, just by describing the dataset and required output.
- start of “no-code AI”
- https://twitter.com/mattshumer_/status/1287125015528341506?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1287125015528341506%7Ctwgr%5E&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Fopenai-gpt-3-model-machine-learning-products%2F 

##### GPT-3 can create designs for apps - Jordan Singer
- future designers can work without coding skills
- https://twitter.com/jsngr/status/1284511080715362304?lang=en

##### Making two GPT-3 AIs chat each other on Stock Market, but they ended up talking about human limitations - Nikita Jerschow
- a mean to predict the future
- https://twitter.com/nikita_jerschow/status/1283755514061520898 

##### Layout Generator that can give you a JSX HTML code - Sharik Shameem
- Sharif Shameem has leveraged GPT-3 to generate code. All he had to do was write a couple of samples to give some context to GPT-3. Shareen tweeted that If he wanted to write output plain HTML/CSS instead of JSX, all he would have to do would be to rewrite the two initial samples in HTML/CSS. Then all of GPT-3’s outputs would be in plain HTML/CSS.
- https://twitter.com/sharifshameem/status/1282676454690451457 

##### GPT-3 can describe a Python code for you in simple English - Amjad Masad
- Amjad Masad, CEO of Replit, built an application where one can type queries about the code on the go and learn information about the functionality of each item in the code.
- https://twitter.com/amasad/status/1285789362647478272?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1285789362647478272%7Ctwgr%5E&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Fopenai-gpt-3-model-machine-learning-products%2F 

## GPT-3 Strengths and Weaknesses
##### Strengths
- Does not require fine-tuning
- High quality generated text
- Does not focus on one specific task like other language model do
- It is the largest trained AI model
- Highly accessible
##### Weaknesses
- Require a lot of resources(285k CPU cores and 10k high-end GPUs)
- Cannot write large passages of coherent text due to limited context window
- The model can gradually go off-topic, diverge from whatever purpose there was for the writing and become unable to come up with sensible continuations. 
- Ethical and social problems

## References:
- [Link 1] (https://towardsdatascience.com/will-gpt-3-kill-coding-630e4518c04d)
- [Link 2] (https://techcrunch.com/2020/06/11/openai-makes-an-all-purpose-api-for-its-text-based-ai-capabilities/)
- [Link 3] (https://www.technologyreview.com/2020/09/23/1008729/openai-is-giving-microsoft-exclusive-access-to-its-gpt-3-language-model/)
- [Link 4] (https://www.thomsonreuters.com/en/artificial-intelligence/natural-language-processing.html)
- [Link 5] (https://www.zdnet.com/article/openais-gigantic-gpt-3-hints-at-the-limits-of-language-models-for-ai/)
- [Link 6] (https://analyticsindiamag.com/open-ai-gpt-3-language-model/)
